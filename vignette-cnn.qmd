---
title: "vignette-cnn.qmd"
format: html
editor: visual
---

Authors: Lucas Childs

## Conceptual Overview

### Lucas' Section

### Janice's Section

**Why comparing to traditional NN, CNN work better in our case**

In a traditional fully connected neural network (NN), each neuron in one layer connects to every neuron in the next layer, leading to an explosion of parameters. For high-dimensional data like genomic data or medical images (this is the exact case for our project), this will be computationally infeasible and prone to overfitting. However, the **convolution** operation solves this through **parameter sharing**. Instead of learning separate weights for every pixel position, a convolutional layer uses a small set of learnable filters (or kernels) that are slid across the entire input image.

Specifically, in our brain tumor detection context, this parameter sharing feature allows the CNN model to efficiently learn fundamental visual patterns from MRI slices without requiring an impossibly large dataset or computational power. It enforces the model to focus on what features are present rather than memorizing their exact spatial coordinates.

**Hierarchical Feature Learning**

A CNN model does not process an image all at one time. Instead, it builds a hierarchical representation through successive layers, imitating the progressive complexity of the human visual system.

## Code Demo

### Kaeya's Section

up to line 176

### Sophie's Section

### Evaluating the Model

```{r}
scores <- model$evaluate(test_flow)

# Convert Python evaluation output to R list
scores_r <- py_to_r(scores)
test_loss <- scores_r[[1]]
test_accuracy <- scores_r[[2]]

cat("Test loss:", test_loss, "\n")
cat("Test accuracy:", test_accuracy, "\n")
```

*This section evaluates the trained CNN on the test dataset to measure how well it generalizes to unseen images. The model\$evaluate() function computes two key metrics: test loss, which measures overall prediction error, and test accuracy, which measures the proportion of correctly classified images. By converting the Python output into an R list, we extract and print these values. A high accuracy value of 0.9075081 and generally low loss value of 0.2428377 indicate that the model has learned meaningful patterns from the training data and performs reliably on new samples from the test data.*

### Confusion Matrix

```{r}
# Get predicted probabilities
pred_probs <- model$predict(test_flow)

# Convert probabilities to class labels (0 or 1)
pred_labels <- ifelse(pred_probs >= 0.5, 1, 0)

true_labels <- test_flow$classes 


cm <- caret::confusionMatrix(factor(pred_labels), factor(true_labels), 
                             positive = "1")

# Plot confusion matrix heatmap using ggplot
cm_df <- as.data.frame(cm_table)
colnames(cm_df) <- c("Predicted", "Actual", "Freq")


p_cm <- ggplot(cm_df, aes(x = Actual, y = Predicted, fill = Freq)) +
  geom_tile() +
  geom_text(aes(label = Freq), color = "white", size = 6) +
  labs(title = "Confusion Matrix", x = "Actual", y = "Predicted") +
  theme_minimal()
p_cm


ggsave("img/confusion_matrix.png", plot = p_cm, width = 6, height = 5)
```

*Along with computing test accuracy and loss, we can also assess model performance at the prediction level by examining the confusion matrix, which shows how many images were correctly or incorrectly classified. First, predicted probabilities are converted into binary labels (0 or 1 for tumor vs. no tumor), and these are compared to the true labels from the test set. The caret::confusionMatrix() function provides precision, recall, sensitivity, specificity, and an overall accuracy score. To make the results easier to visualize, we convert the confusion matrix into a dataframe and generate a heatmap with ggplot2. This visualization highlights where the model succeeds and where it makes mistakes, helping diagnose model weaknesses or class imbalance. As we can see from the visualization, our model correctly identified 489 images with tumors, and 345 images without tumors. Additionally, the model only misclassified a small amount of images, incorrectly classifying 72 images to have a tumor when they didn't, and 13 images to not have a tumor when they in fact did. The confusion matrix indicates that the model correctly classified the majority of images, resulting in a high overall accuracy despite some misclassifications. Moreover, in a medical context, the model’s tendency to flag more false positives than false negatives is preferable, as it errs on the side of caution by reducing the risk of missing actual tumor cases.*

### ROC Curve and AUC

```{r}
roc_obj <- roc(response = true_labels, predictor = as.numeric(pred_probs))
auc_val <- auc(roc_obj)
cat("AUC:", auc_val, "\n")
# 0.9750686


png("img/roc_curve.png", width = 900, height = 700)
plot(roc_obj)
```

*The ROC curve provides a visual summary of the model’s ability to distinguish between tumor and non-tumor images across all possible classification thresholds. By plotting sensitivity (true positive rate) against 1 – specificity (false positive rate), the ROC curve illustrates how the model’s performance changes as the decision threshold shifts, highlighting the trade-off between capturing more true tumors and avoiding false alarms. The curve demonstrates strong separation between the two classes, and the resulting AUC value of 0.975 indicates excellent discriminatory performance, meaning the model can correctly rank tumor images above non-tumor images with very high probability.*

# Saving the Model

```{r}
dir_create("results")
model$save("results/cnn_brain_tumor_model.keras")

cat("Model saved to results/cnn_brain_tumor_model.h5\n")
```

*Finally, we save the trained CNN model so it can be reused without needing to retrain it. We created a results/ folder and save the model in .keras format, which preserves both the architecture and the trained weights. Saving the model ensures that it can be loaded later for inference, further training, or deployment in a clinical or research workflow. This also makes the results reproducible and portable across systems.*
