---
title: "janice"
author: "Janice Jiang"
date: "2025-12-02"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)

library(keras)
library(tensorflow)
library(ggplot2)
library(caret)
library(dplyr)
library(reticulate)
library(pROC)
library(readr)
library(fs)
library(rsample)
library(tidyverse)

# Set random seeds for reproducibility
set.seed(11272025)
tensorflow::set_random_seed(11272025)

tryCatch({
  pd <- import("pandas")
  cat("Pandas is available in Python environment\n")
}, error = function(e) {
  cat("Pandas is NOT available. Installing...\n")
  # Try to install pandas
  tryCatch({
    virtualenv_install("r-tensorflow", "pandas")
    cat("Pandas installed successfully\n")
  }, error = function(e2) {
    cat("Could not install pandas automatically. Please run:\n")
    cat("reticulate::virtualenv_install('pandas', envname = 'r-tensorflow')\n")
    cat("Then restart R session and try again.\n")
    stop("Pandas installation required")
  })
})
```

```{r}
# 1. DATA LOADING AND PREPARATION
# =================================

# Load metadata
metadata <- read_csv("data/metadata.csv.xls") 

cat("Dataset structure:\n")
str(metadata)
cat("\nFirst few rows:\n")
head(metadata)
cat("\nColumn names:\n")
colnames(metadata)

cat("\nClass distribution:\n")
table(metadata$class)

metadata <- metadata %>%
  mutate(class = if_else(class == "tumor", "Cancer", "Not Cancer"))

# training/testing
train_index <- createDataPartition(metadata$class, p = 0.8, list = FALSE)

train_data <- metadata[train_index, ]
test_data  <- metadata[-train_index, ]

cat("\nTraining set size:", nrow(train_data))
cat("\nTest set size:", nrow(test_data))
```

```{r}
# 2. CREATE DIRECTORY STRUCTURE FOR IMAGES
# ========================================

dir_create("data/train_images", showWarnings = FALSE)
dir_create("data/test_images", showWarnings = FALSE)

# Copy training images
for (i in 1:nrow(train_data)) {
  img_file <- paste0("data/ori_image/", train_data$image[i])
  if (file_exists(img_file)) {
    file_copy(
      path = img_file,
      new_path = paste0("data/train_images/", train_data$image[i])
    )
  }
}
  
# Copy test images
for (i in 1:nrow(test_data)) {
  img_file <- paste0("data/ori_image/", test_data$image[i])
  if (file_exists(img_file)) {
    file_copy(
      path = img_file,
      new_path = paste0("data/test_images/", test_data$image[i])
    )
  }
}
```

```{r}
# 3. CREATE IMAGE GENERATORS
# ================================================

# Set image parameters
img_height <- 224
img_width  <- 224
batch_size <- 32

train_datagen <- image_data_generator(rescale = 1/255)
test_datagen <- image_data_generator(rescale = 1/255)

# Create training data flow
train_flow <- flow_images_from_dataframe(
  dataframe = train_data,
  directory = "data/train_images",
  x_col = "image",
  y_col = "class",
  generator = train_datagen,
  target_size = c(img_height, img_width),
  batch_size = batch_size,
  class_mode = "binary",
  shuffle = TRUE,
  seed = 11272025
)

# Create test data flow
test_flow <- flow_images_from_dataframe(
  dataframe = test_data,
  directory = "data/test_images",
  x_col = "image",
  y_col = "class",
  generator = test_datagen,
  target_size = c(img_height, img_width),
  batch_size = batch_size,
  class_mode = "binary",
  shuffle = FALSE
)
```

```{r}
# 4. BUILD CNN MODEL
# ==================

cnn_model <- keras_model_sequential(list(
  # 32 convolution filters=
  layer_conv_2d(filters = 32, kernel_size = 3, activation = "relu",
                input_shape = c(img_height, img_width, 3)),
  layer_max_pooling_2d(pool_size = 2),
  # 64 filters
  layer_conv_2d(filters = 64, kernel_size = 3, activation = "relu"),
  layer_max_pooling_2d(pool_size = 2),
  # 128 filters
  layer_conv_2d(filters = 128, kernel_size = 3, activation = "relu"),
  layer_max_pooling_2d(pool_size = 2),
  # Flatten and dense layers
  layer_flatten(),
  layer_dense(units = 128, activation = "relu"),
  layer_dropout(rate = 0.4),
  # Output layer
  layer_dense(units = 1, activation = "sigmoid")
))

# Display model summary
cat("\n=== Model Summary ===\n")
summary(model)

# 5. COMPILE MODEL
# ================
model$compile(
  loss = "binary_crossentropy",
  optimizer = optimizer_adam(learning_rate = 0.0001),
  metrics = list("accuracy")
)
```

```{r}
# 6. TRAIN THE MODEL
# ==================
epochs <- 25

# Calculate steps per epoch
train_steps <- as.integer(ceiling(nrow(train_data) / batch_size))
test_steps <- as.integer(ceiling(nrow(test_data) / batch_size))

# Create callbacks
callbacks <- list(
  callback_early_stopping(
    monitor = "val_accuracy",
    patience = 5,
    restore_best_weights = TRUE,
    verbose = 1
  ),
  callback_reduce_lr_on_plateau(
    monitor = "val_loss",
    factor = 0.5,
    patience = 3,
    verbose = 1
  )
)

# Train the model
cat("\n=== Starting Training ===\n")
history <- model$fit(
   x = train_flow,
   steps_per_epoch = train_steps,
   epochs = as.integer(epochs),
   validation_data = test_flow,
   validation_steps = test_steps,
   callbacks = callbacks,
   verbose = 1)
```

```{r}
# 7. VISUALIZE TRAINING HISTORY
# =============================
history_df <- as.data.frame(history$history) %>% mutate(epoch = 1:19)

# Plot training and validation loss
p_loss <- ggplot(history_df, aes(x = epoch)) +
  geom_line(aes(y = loss, color = "Training Loss"), size = 1) +
  geom_line(aes(y = val_loss, color = "Validation Loss"), size = 1) +
  geom_point(aes(y = loss, color = "Training Loss"), size = 2) +
  geom_point(aes(y = val_loss, color = "Validation Loss"), size = 2) +
  geom_vline(xintercept = 14, color = "red", linetype = "dashed", size = 1) +
  labs(title = "Model Loss", 
       x = "Epoch", 
       y = "Loss") +
  theme_minimal(base_size = 12) +
  theme(legend.position = "bottom",
        plot.title = element_text(hjust = 0.5)) +
  scale_color_manual(name = "",
                     values = c("Training Loss" = "skyblue", 
                                "Validation Loss" = "orange"))

# Plot training and validation accuracy
p_acc <- ggplot(history_df, aes(x = epoch)) +
  geom_line(aes(y = accuracy, color = "Training Accuracy"), size = 1) +
  geom_line(aes(y = val_accuracy, color = "Validation Accuracy"), size = 1) +
  geom_point(aes(y = accuracy, color = "Training Accuracy"), size = 2) +
  geom_point(aes(y = val_accuracy, color = "Validation Accuracy"), size = 2) +
  geom_vline(xintercept = 14, color = "red", linetype = "dashed", size = 1) +
  labs(title = "Model Accuracy", 
       x = "Epoch", 
       y = "Accuracy") +
  theme_minimal(base_size = 12) +
  theme(legend.position = "bottom",
        plot.title = element_text(hjust = 0.5)) +
  scale_color_manual(name = "",
                     values = c("Training Accuracy" = "skyblue", 
                                "Validation Accuracy" = "orange"))

# Display plots
print(p_loss)
print(p_acc)
```

```{r}
ggsave("results/training_loss.png", 
       p_loss, width = 6, height = 4)
ggsave("results/training_accuracy.png", 
       p_acc, width = 6, height = 4)
```

```{r}
# 8. EVALUATE THE MODEL
# =====================
# Load the best model if early stopping saved it
if (file_exists("results/janice_cnn_model.keras")) {
  best_model <- load_model_tf("results/janice_cnn_model.keras")
  cat("\nLoaded best model from checkpoint\n")
} else {
  best_model <- model
}

# Evaluate on test data
cat("\n=== Model Evaluation ===\n")
evaluation <- best_model$evaluate(test_flow, steps = test_steps, verbose = 0)
cat("Test Loss:", round(py_to_r(evaluation)[[1]], 4), "\n")
cat("Test Accuracy:", round(py_to_r(evaluation)[[2]], 4), "\n")
```

```{r}
# 9. MAKE PREDICTIONS AND CONFUSION MATRIX
# =========================================

# Get predictions
cat("\nMaking predictions...\n")
predictions <- best_model$predict(test_flow, steps = test_steps, verbose = 0)

# Convert probabilities to binary predictions
predicted_classes <- ifelse(predictions > 0.5, "Not Cancer", "Cancer")

# Get true labels
true_classes <- test_data$class

# Create confusion matrix
conf_matrix <- confusionMatrix(
  factor(predicted_classes),
  factor(true_classes)
)

cat("\n=== Confusion Matrix ===\n")
print(conf_matrix)
```

```{r}
accuracy <- conf_matrix$overall['Accuracy']
precision <- conf_matrix$byClass['Precision']
recall <- conf_matrix$byClass['Recall']
f1_score <- conf_matrix$byClass['F1']

cat("\n=== Detailed Metrics ===\n")
cat("Accuracy:", round(accuracy * 100, 2), "%\n")
cat("Precision:", round(precision * 100, 2), "%\n")
cat("Recall:", round(recall * 100, 2), "%\n")
cat("F1-Score:", round(f1_score * 100, 2), "%\n")
```

```{r}
best_model$save("results/janice_cnn_model.keras")
```

