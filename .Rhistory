library(keras3)
library(tensorflow)
library(keras3)
install_tensorflow(
method = "virtualenv",
envname = "r-tensorflow",
version = "2.13.0"  # stable version compatible with R keras3
)
library(reticulate)
use_virtualenv("r-tensorflow", required = TRUE)
library(keras3)
library(tensorflow)
tf$constant("Hello TensorFlow")
py_require("tensorflow")
tf$constant("Hello TensorFlow")
model %>% compile(
optimizer = "adam",
loss = "binary_crossentropy",
metrics = "accuracy"
)
model
# Creates a sequential model where layers are stacked in order
model <- keras_model_sequential() %>%
# 32 convolution filters of size 3x3
# looks for local patterns in the image
# max pooling downsamples the feature maps
layer_conv_2d(filters = 32, kernel_size = 3, activation = "relu",
input_shape = c(img_height, img_width, 3)) %>%
layer_max_pooling_2d(pool_size = 2) %>%
# expands filters to 64
# extracts more complex spacial features
# pooling reduces dimensionality
layer_conv_2d(filters = 64, kernel_size = 3, activation = "relu") %>%
layer_max_pooling_2d(pool_size = 2) %>%
# even deeper representation with 128 filters
# deeper layers capture high-level features (shapes, textures)
layer_conv_2d(filters = 128, kernel_size = 3, activation = "relu") %>%
layer_max_pooling_2d(pool_size = 2) %>%
# flatten() converts the 3D feature maps into a 1D vector
# dense layer learns global patterns
# dropout randomly removes 40% of neurons during training to reduce overfitting
layer_flatten() %>%
layer_dense(units = 128, activation = "relu") %>%
layer_dropout(0.4) %>%
# 1 output neuron
layer_dense(units = 1, activation = "sigmoid")
# flow images from training dataframe
# read images in batches from the folder train_images
train_flow <- flow_images_from_dataframe(
dataframe = train_data,
directory = "data/train_images",
x_col = "image",
y_col = "class",
generator = train_gen,
target_size = c(img_height, img_width),
batch_size = batch_size,
class_mode = "binary"
)
# flow images from training dataframe
# read images in batches from the folder train_images
train_flow <- dataset_from_dataframe(
dataframe = train_data,
directory = "data/train_images",
x_col = "image",
y_col = "class",
generator = train_gen,
target_size = c(img_height, img_width),
batch_size = batch_size,
class_mode = "binary"
)
library(keras3)
# flow images from training dataframe
# read images in batches from the folder train_images
train_flow <- dataset_from_dataframe(
dataframe = train_data,
directory = "data/train_images",
x_col = "image",
y_col = "class",
generator = train_gen,
target_size = c(img_height, img_width),
batch_size = batch_size,
class_mode = "binary"
)
# flow images from training dataframe
# read images in batches from the folder train_images
train_flow <- flow_images_from_dataframe(
dataframe = train_data,
directory = "data/train_images",
x_col = "image",
y_col = "class",
generator = train_gen,
target_size = c(img_height, img_width),
batch_size = batch_size,
class_mode = "binary"
)
library(keras3)
library(tensorflow)
library(ggplot2)
library(caret)
library(dplyr)
library(reticulate)
library(pROC)
library(readr)
library(fs)
library(rsample)
library(tidyverse)
# load the data
metadata <- read_csv("data/metadata.csv.xls")
# preview the data and column labels
head(metadata)
colnames(metadata)
# setting a seed for reproducibility
set.seed(11272025)
# split data with 80% of the data in the training set and 20% in test set
train_index <- createDataPartition(metadata$class, p = 0.8, list = FALSE)
train_data <- metadata[train_index, ]
test_data  <- metadata[-train_index, ]
# Check sizes
nrow(train_data)
nrow(test_data)
# Create folders for test and training images
dir_create("data/images", showWarnings = FALSE)
dir_create("data/train_images")
dir_create("data/test_images")
# Copy each file based on split into training and test sets
file_copy(
path = paste0("data/images/", train_data$image),
new_path = paste0("data/train_images/", train_data$image)
)
file_copy(
path = paste0("data/images/", test_data$image),
new_path = paste0("data/test_images/", test_data$image)
)
# Sets the target image size after resizing (224×224)
# batch_size = number of images processed in each training step
img_height <- 224
img_width  <- 224
batch_size <- 32
# training generator that loads images and applies augmentation
# this augmentation prevents overfitting by generating diverse images
train_gen <- image_data_generator(
rescale = 1/255,
rotation_range = 10,
width_shift_range = 0.1,
height_shift_range = 0.1,
horizontal_flip = TRUE
)
library(keras)
library(tensorflow)
library(ggplot2)
library(caret)
library(dplyr)
library(reticulate)
library(pROC)
library(readr)
library(fs)
library(rsample)
library(tidyverse)
# load the data
metadata <- read_csv("data/metadata.csv.xls")
# preview the data and column labels
head(metadata)
colnames(metadata)
# setting a seed for reproducibility
set.seed(11272025)
# split data with 80% of the data in the training set and 20% in test set
train_index <- createDataPartition(metadata$class, p = 0.8, list = FALSE)
# training generator that loads images and applies augmentation
# this augmentation prevents overfitting by generating diverse images
train_gen <- image_data_generator(
rescale = 1/255,
rotation_range = 10,
width_shift_range = 0.1,
height_shift_range = 0.1,
horizontal_flip = TRUE
)
# create testing generator with no augmentation of images
test_gen <- image_data_generator(rescale = 1/255)
# flow images from training dataframe
# read images in batches from the folder train_images
train_flow <- flow_images_from_dataframe(
dataframe = train_data,
directory = "data/train_images",
x_col = "image",
y_col = "class",
generator = train_gen,
target_size = c(img_height, img_width),
batch_size = batch_size,
class_mode = "binary"
)
reticulate::virtualenv_install("r-tensorflow", packages = "pandas")
library(reticulate)
py_module_available("pandas")
# flow images from training dataframe
# read images in batches from the folder train_images
train_flow <- flow_images_from_dataframe(
dataframe = train_data,
directory = "data/train_images",
x_col = "image",
y_col = "class",
generator = train_gen,
target_size = c(img_height, img_width),
batch_size = batch_size,
class_mode = "binary"
)
# flow images from the test dataframe
test_flow <- flow_images_from_dataframe(
dataframe = test_data,
directory = "data/test_images",
x_col = "image",
y_col = "class",
generator = test_gen,
target_size = c(img_height, img_width),
batch_size = batch_size,
class_mode = "binary",
shuffle = FALSE
)
# Creates a sequential model where layers are stacked in order
model <- keras_model_sequential() %>%
# 32 convolution filters of size 3x3
# looks for local patterns in the image
# max pooling downsamples the feature maps
layer_conv_2d(filters = 32, kernel_size = 3, activation = "relu",
input_shape = c(img_height, img_width, 3)) %>%
layer_max_pooling_2d(pool_size = 2) %>%
# expands filters to 64
# extracts more complex spacial features
# pooling reduces dimensionality
layer_conv_2d(filters = 64, kernel_size = 3, activation = "relu") %>%
layer_max_pooling_2d(pool_size = 2) %>%
# even deeper representation with 128 filters
# deeper layers capture high-level features (shapes, textures)
layer_conv_2d(filters = 128, kernel_size = 3, activation = "relu") %>%
layer_max_pooling_2d(pool_size = 2) %>%
# flatten() converts the 3D feature maps into a 1D vector
# dense layer learns global patterns
# dropout randomly removes 40% of neurons during training to reduce overfitting
layer_flatten() %>%
layer_dense(units = 128, activation = "relu") %>%
layer_dropout(0.4) %>%
# 1 output neuron
layer_dense(units = 1, activation = "sigmoid")
model %>% compile(
optimizer = "adam",
loss = "binary_crossentropy",
metrics = "accuracy"
)
model
# Train CNN
epochs <- 15
history <- model %>% fit(
train_flow,
steps_per_epoch = ceiling(nrow(train_data) / batch_size),
validation_data = test_flow,
validation_steps = ceiling(nrow(test_data) / batch_size),
epochs = epochs
)
reticulate::virtualenv_install("r-tensorflow", packages = "Pillow")
py_module_available("PIL")
library(keras)
library(tensorflow)
library(ggplot2)
library(caret)
library(dplyr)
library(reticulate)
library(pROC)
library(readr)
library(fs)
library(rsample)
library(tidyverse)
# load the data
metadata <- read_csv("data/metadata.csv.xls")
# preview the data and column labels
head(metadata)
colnames(metadata)
# setting a seed for reproducibility
set.seed(11272025)
# split data with 80% of the data in the training set and 20% in test set
train_index <- createDataPartition(metadata$class, p = 0.8, list = FALSE)
train_data <- metadata[train_index, ]
test_data  <- metadata[-train_index, ]
# Check sizes
nrow(train_data)
nrow(test_data)
# Create folders for test and training images
dir_create("data/images", showWarnings = FALSE)
dir_create("data/train_images")
dir_create("data/test_images")
# Copy each file based on split into training and test sets
file_copy(
path = paste0("data/images/", train_data$image),
new_path = paste0("data/train_images/", train_data$image)
)
file_copy(
path = paste0("data/images/", test_data$image),
new_path = paste0("data/test_images/", test_data$image)
)
# Sets the target image size after resizing (224×224)
# batch_size = number of images processed in each training step
img_height <- 224
img_width  <- 224
batch_size <- 32
# training generator that loads images and applies augmentation
# this augmentation prevents overfitting by generating diverse images
train_gen <- image_data_generator(
rescale = 1/255,
rotation_range = 10,
width_shift_range = 0.1,
height_shift_range = 0.1,
horizontal_flip = TRUE
)
# create testing generator with no augmentation of images
test_gen <- image_data_generator(rescale = 1/255)
# flow images from training dataframe
# read images in batches from the folder train_images
train_flow <- flow_images_from_dataframe(
dataframe = train_data,
directory = "data/train_images",
x_col = "image",
y_col = "class",
generator = train_gen,
target_size = c(img_height, img_width),
batch_size = batch_size,
class_mode = "binary"
)
# flow images from the test dataframe
test_flow <- flow_images_from_dataframe(
dataframe = test_data,
directory = "data/test_images",
x_col = "image",
y_col = "class",
generator = test_gen,
target_size = c(img_height, img_width),
batch_size = batch_size,
class_mode = "binary",
shuffle = FALSE
)
# Creates a sequential model where layers are stacked in order
model <- keras_model_sequential() %>%
# 32 convolution filters of size 3x3
# looks for local patterns in the image
# max pooling downsamples the feature maps
layer_conv_2d(filters = 32, kernel_size = 3, activation = "relu",
input_shape = c(img_height, img_width, 3)) %>%
layer_max_pooling_2d(pool_size = 2) %>%
# expands filters to 64
# extracts more complex spacial features
# pooling reduces dimensionality
layer_conv_2d(filters = 64, kernel_size = 3, activation = "relu") %>%
layer_max_pooling_2d(pool_size = 2) %>%
# even deeper representation with 128 filters
# deeper layers capture high-level features (shapes, textures)
layer_conv_2d(filters = 128, kernel_size = 3, activation = "relu") %>%
layer_max_pooling_2d(pool_size = 2) %>%
# flatten() converts the 3D feature maps into a 1D vector
# dense layer learns global patterns
# dropout randomly removes 40% of neurons during training to reduce overfitting
layer_flatten() %>%
layer_dense(units = 128, activation = "relu") %>%
layer_dropout(0.4) %>%
# 1 output neuron
layer_dense(units = 1, activation = "sigmoid")
model %>% compile(
optimizer = "adam",
loss = "binary_crossentropy",
metrics = "accuracy"
)
model
# Train CNN
epochs <- 15
history <- model %>% fit(
train_flow,
steps_per_epoch = ceiling(nrow(train_data) / batch_size),
validation_data = test_flow,
validation_steps = ceiling(nrow(test_data) / batch_size),
epochs = epochs
)
reticulate::virtualenv_install("r-tensorflow", packages = c("scipy", "pandas", "Pillow"))
library(keras)
library(tensorflow)
library(ggplot2)
library(caret)
library(dplyr)
library(reticulate)
library(pROC)
library(readr)
library(fs)
library(rsample)
library(tidyverse)
# load the data
metadata <- read_csv("data/metadata.csv.xls")
# preview the data and column labels
head(metadata)
colnames(metadata)
# setting a seed for reproducibility
set.seed(11272025)
# split data with 80% of the data in the training set and 20% in test set
train_index <- createDataPartition(metadata$class, p = 0.8, list = FALSE)
train_data <- metadata[train_index, ]
test_data  <- metadata[-train_index, ]
# Check sizes
nrow(train_data)
nrow(test_data)
# Create folders for test and training images
dir_create("data/images", showWarnings = FALSE)
dir_create("data/train_images")
dir_create("data/test_images")
# Copy each file based on split into training and test sets
file_copy(
path = paste0("data/images/", train_data$image),
new_path = paste0("data/train_images/", train_data$image)
)
file_copy(
path = paste0("data/images/", test_data$image),
new_path = paste0("data/test_images/", test_data$image)
)
# Sets the target image size after resizing (224×224)
# batch_size = number of images processed in each training step
img_height <- 224
img_width  <- 224
batch_size <- 32
# training generator that loads images and applies augmentation
# this augmentation prevents overfitting by generating diverse images
train_gen <- image_data_generator(
rescale = 1/255,
rotation_range = 10,
width_shift_range = 0.1,
height_shift_range = 0.1,
horizontal_flip = TRUE
)
# training generator that loads images and applies augmentation
# this augmentation prevents overfitting by generating diverse images
train_gen <- image_data_generator(
rescale = 1/255,
rotation_range = 10,
width_shift_range = 0.1,
height_shift_range = 0.1,
horizontal_flip = TRUE
)
`py_require("tensorflow")`
py_require("tensorflow")
# training generator that loads images and applies augmentation
# this augmentation prevents overfitting by generating diverse images
train_gen <- image_data_generator(
rescale = 1/255,
rotation_range = 10,
width_shift_range = 0.1,
height_shift_range = 0.1,
horizontal_flip = TRUE
)
library(keras)
library(tensorflow)
library(ggplot2)
library(caret)
library(dplyr)
library(reticulate)
library(pROC)
library(readr)
library(fs)
library(rsample)
library(tidyverse)
# load the data
metadata <- read_csv("data/metadata.csv.xls")
# preview the data and column labels
head(metadata)
colnames(metadata)
# setting a seed for reproducibility
set.seed(11272025)
# split data with 80% of the data in the training set and 20% in test set
train_index <- createDataPartition(metadata$class, p = 0.8, list = FALSE)
train_data <- metadata[train_index, ]
test_data  <- metadata[-train_index, ]
# Check sizes
nrow(train_data)
nrow(test_data)
# Create folders for test and training images
dir_create("data/images", showWarnings = FALSE)
dir_create("data/train_images")
dir_create("data/test_images")
# Copy each file based on split into training and test sets
file_copy(
path = paste0("data/images/", train_data$image),
new_path = paste0("data/train_images/", train_data$image)
)
file_copy(
path = paste0("data/images/", test_data$image),
new_path = paste0("data/test_images/", test_data$image)
)
# Sets the target image size after resizing (224×224)
# batch_size = number of images processed in each training step
img_height <- 224
img_width  <- 224
batch_size <- 32
# training generator that loads images and applies augmentation
# this augmentation prevents overfitting by generating diverse images
train_gen <- image_data_generator(
rescale = 1/255,
rotation_range = 10,
width_shift_range = 0.1,
height_shift_range = 0.1,
horizontal_flip = TRUE
)
# Install TensorFlow (this includes Keras)
virtualenv_install("r-tensorflow", packages = c("tensorflow", "keras", "numpy", "pandas", "Pillow", "scipy"))
library(reticulate)
use_virtualenv("r-tensorflow", required = TRUE)
library(keras)
tf$constant("Hello TensorFlow")
tf$constant("Hello TensorFlow")
py_require("tensorflow")
py_require("tensorflow")
tf$constant("Hello TensorFlow")
